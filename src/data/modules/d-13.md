# Centralized Logging Concepts

## 1. Orientation

### What this module covers
This module covers centralized logging architecture—how organizations collect, process, store, and analyze logs from hundreds or thousands of sources. This infrastructure is the foundation of security monitoring.

### Fit in the Learning Path
You cannot SSH into 5,000 servers to check logs daily. Centralized logging provides a single pane of glass for security visibility, enabling correlation, alerting, and investigation at scale.

### Learning Objectives
By the end of this module, you will:
*   Understand log collection and aggregation architecture
*   Apply normalization and parsing concepts
*   Recognize major SIEM platforms
*   Design correlation rules for detection

---

## 2. Core Content

### Mental Model: The Logging Pipeline

**How professionals think about this:**
Centralized logging is a data pipeline. Logs flow from sources through collection, enrichment, storage, and finally to analysis. Each stage has trade-offs between completeness, cost, and performance.

```
CENTRALIZED LOGGING PIPELINE:

┌─────────────────────────────────────────────────────────────────────┐
│                                                                     │
│  SOURCES                                                           │
│  ───────                                                           │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐      │
│  │ Windows │ │  Linux  │ │Firewalls│ │   Apps  │ │  Cloud  │      │
│  │ Servers │ │ Servers │ │ Routers │ │  DBs    │ │ AWS/GCP │      │
│  └────┬────┘ └────┬────┘ └────┬────┘ └────┬────┘ └────┬────┘      │
│       │          │          │          │          │              │
│       ▼          ▼          ▼          ▼          ▼              │
│  COLLECTION (Shippers/Agents)                                     │
│  ─────────────────────────────                                    │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │ Filebeat, Winlogbeat, Splunk Forwarder, Fluentd, NXLog      │ │
│  └──────────────────────────────────────┬───────────────────────┘ │
│                                         │                         │
│                                         ▼                         │
│  TRANSPORT/BUFFER (Optional)                                      │
│  ────────────────────────────                                     │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │ Kafka, Redis, RabbitMQ                                       │ │
│  │ Handles bursts, prevents data loss                           │ │
│  └──────────────────────────────────────┬───────────────────────┘ │
│                                         │                         │
│                                         ▼                         │
│  PROCESSING (Parsing, Enrichment)                                │
│  ────────────────────────────────                                │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │ Logstash, Cribl, Splunk Indexer                             │ │
│  │ Parse → Normalize → Enrich → Route                          │ │
│  └──────────────────────────────────────┬───────────────────────┘ │
│                                         │                         │
│                                         ▼                         │
│  STORAGE (Index)                                                 │
│  ───────────────                                                 │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │ Elasticsearch, Splunk, Azure Sentinel, Chronicle            │ │
│  │ Indexed for fast searching                                  │ │
│  └──────────────────────────────────────┬───────────────────────┘ │
│                                         │                         │
│                                         ▼                         │
│  ANALYSIS (Visualization & Alerting)                             │
│  ───────────────────────────────────                             │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │ Kibana, Splunk Web, Grafana                                 │ │
│  │ Dashboards, Queries, Alerts, Reports                        │ │
│  └──────────────────────────────────────────────────────────────┘ │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### Collection Agents

**Shipping logs to central location:**

| Agent | Platform | Protocol | Use Case |
|-------|----------|----------|----------|
| **Filebeat** | Any | TCP/HTTP | File-based logs |
| **Winlogbeat** | Windows | TCP/HTTP | Windows Events |
| **Sysmon** + Winlogbeat | Windows | TCP/HTTP | Enhanced Windows |
| **Splunk Universal Forwarder** | Any | Splunk Protocol | Splunk environments |
| **Fluentd/Fluent Bit** | Any | Multiple | Cloud-native/K8s |
| **NXLog** | Any | Multiple | Cross-platform |
| **rsyslog** | Linux | Syslog | Linux standard |

**Agent considerations:**

```
AGENT DEPLOYMENT CONCERNS:

1. RESOURCE USAGE
   - CPU/Memory footprint
   - Network bandwidth
   - Impact on production systems

2. RELIABILITY
   - What happens if central server is down?
   - Local buffering? Data loss?
   
3. SECURITY
   - Agent authentication
   - Encrypted transport
   - Agent as attack surface

4. COVERAGE
   - Which systems have agents?
   - What about ephemeral containers?
   - OT/IoT devices without agent support?
```

### Normalization

**Converting chaos to order:**

```
NORMALIZATION PROBLEM:

SAME EVENT, DIFFERENT FORMATS:

Windows:
EventID: 4624
AccountName: alice
LogonType: 10
SourceIP: 192.168.1.50

Linux:
Accepted password for alice from 192.168.1.50 port 55555 ssh2

AWS CloudTrail:
"userIdentity": {"userName": "alice"},
"eventName": "ConsoleLogin",
"sourceIPAddress": "192.168.1.50"

AFTER NORMALIZATION:

{
  "event_type": "authentication",
  "outcome": "success",
  "user": "alice",
  "source_ip": "192.168.1.50",
  "method": "interactive",
  "timestamp": "2024-01-15T08:30:00Z"
}

NOW YOU CAN:
Query: event_type:authentication AND user:alice
And get results from ALL sources
```

**Common normalized fields:**

| Field | Description |
|-------|-------------|
| `timestamp` | When (UTC) |
| `source_ip` | Where from |
| `dest_ip` | Where to |
| `user` | Who |
| `event_type` | What category |
| `action` | What happened |
| `outcome` | Success/Failure |
| `process_name` | Which program |
| `file_path` | Which file |

### SIEM Platforms

**Major solutions:**

```
SIEM LANDSCAPE:

ENTERPRISE/COMMERCIAL:
─────────────────────────────
Splunk Enterprise
├── Market leader
├── Powerful search language (SPL)
├── Expensive (GB/day pricing)
└── On-prem or cloud

Microsoft Sentinel
├── Azure-native
├── KQL query language
├── Integrated with M365/Azure
└── Cloud-only

IBM QRadar
├── Enterprise-focused
├── Strong compliance features
└── On-prem or cloud

OPEN SOURCE / COST-EFFECTIVE:
─────────────────────────────
ELK Stack (Elastic)
├── Elasticsearch + Logstash + Kibana
├── Free core, paid features
├── Very flexible
└── Requires expertise to operate

Graylog
├── User-friendly
├── Good for mid-size
└── Open core model

Wazuh
├── Security-focused (based on OSSEC)
├── Built-in rules
├── Good for compliance
└── Free and open source

CLOUD/MODERN:
─────────────────────────────
Google Chronicle
├── Google infrastructure
├── 1 year hot retention
├── Security-focused

Sumo Logic
├── Cloud-native
├── Easy to deploy
└── Pay per usage
```

### Correlation Rules

**Finding attacks in log ocean:**

```
CORRELATION CONCEPT:

Single event:
"User alice logged in" → So what?

Correlated events:
"User alice logged in" → From new country
                      → At 3 AM
                      → After 50 failed attempts
                      → Then accessed finance server
                      
→ ALERT: Possible account compromise with lateral movement

CORRELATION TYPES:
─────────────────────────────
1. THRESHOLD
   IF failed_logins > 10 in 5 minutes
   THEN alert

2. SEQUENCE
   IF failed_login THEN
   FOLLOWED BY success_login within 10 min
   THEN alert "Brute force success"

3. STATISTICAL
   IF login_time differs from user's baseline by > 3 std dev
   THEN alert "Anomalous login time"

4. LOOKBACK
   IF user connects to IP
   AND IP was in threat feed yesterday
   THEN alert
```

**Example Splunk/SPL rules:**

```spl
# Brute force detection
index=auth sourcetype=wineventlog EventCode=4625
| stats count by user, src_ip
| where count > 10

# Impossible travel
index=auth sourcetype=auth_logs action=success
| iplocation src_ip
| stats earliest(Country) as first_country, 
        latest(Country) as last_country,
        earliest(_time) as first_time,
        latest(_time) as last_time
        by user
| where first_country!=last_country 
  AND (last_time - first_time) < 3600

# Beaconing detection
index=network
| bucket _time span=1m
| stats count by src_ip, dest_ip, _time
| transaction src_ip, dest_ip
| where eventcount > 50 AND stdev(count) < 1
```

### Storage and Retention

**Managing log volume:**

```
STORAGE CONSIDERATIONS:

VOLUME CALCULATION:
─────────────────────────────
1,000 endpoints × 10MB/day = 10GB/day
10GB/day × 365 days = 3.65TB/year
3.65TB × 3 years retention = ~11TB

Plus: Firewalls, apps, cloud = Multiply by 5-10x

TIERED STORAGE:
─────────────────────────────
HOT (0-30 days)
├── Fast SSD storage
├── Full indexing
├── Interactive search
└── Expensive

WARM (30-90 days)
├── Slower storage
├── Some indexing
├── Slower search
└── Moderate cost

COLD (90-365 days)
├── Archive storage
├── Minimal indexing
├── Batch search only
└── Cheap

FROZEN (1+ years)
├── S3/Glacier
├── No indexing
├── Compliance only
└── Very cheap

WHAT TO LOG:
─────────────────────────────
Log everything? → Expensive, noisy
Log nothing? → Blind to attacks

Approach:
1. Security events → Full retention
2. Network metadata → Medium retention
3. Debug logs → Short or don't collect
4. Compliance-required → Per regulation
```

### Common Challenges

**Real-world problems:**

```
CENTRALIZED LOGGING CHALLENGES:

1. COST
   ─────────────────────────────
   Splunk: $$$$ per GB ingested
   Solution: Filter at source, tiered storage

2. LATENCY
   ─────────────────────────────
   Logs arrive 5 minutes late
   Real-time detection fails
   Solution: Tune collection intervals, edge processing

3. PARSING FAILURES
   ─────────────────────────────
   Application changed log format
   Parser breaks, logs not indexed properly
   Solution: Schema registry, parser testing

4. FALSE POSITIVES
   ─────────────────────────────
   Too many alerts, analysts ignore them
   Solution: Tuning, baselining, ML-based anomaly

5. COVERAGE GAPS
   ─────────────────────────────
   Some systems not sending logs
   Shadow IT not visible
   Solution: Asset inventory, coverage metrics

6. SKILL GAP
   ─────────────────────────────
   Complex query languages (SPL, KQL)
   Hard to write effective rules
   Solution: Training, pre-built content
```

---

## 3. Guided Practice

### Exercise 1: Pipeline Design

Design a logging pipeline for:
- 500 Windows servers
- 200 Linux servers
- 10 firewalls
- AWS cloud environment

| Component | Tool Choice | Reasoning |
|-----------|-------------|-----------|
| Collection | ___ | ___ |
| Transport | ___ | ___ |
| Processing | ___ | ___ |
| Storage | ___ | ___ |
| Analysis | ___ | ___ |

### Exercise 2: Normalization

Normalize these events to common schema:

Windows Event:
```
EventCode: 4625, AccountName: bob, SourceIP: 10.0.0.50
```

Linux Auth:
```
Failed password for bob from 10.0.0.50 port 22 ssh2
```

| Field | Windows Value | Linux Value |
|-------|---------------|-------------|
| event_type | ___ | ___ |
| user | ___ | ___ |
| source_ip | ___ | ___ |
| outcome | ___ | ___ |

### Exercise 3: Correlation Rule

Write pseudo-code for detecting:
"User logs into 5+ systems within 10 minutes"

```
RULE: rapid_lateral_movement
IF ___
WITHIN ___
THRESHOLD ___
THEN ___
```

---

## 4. Reflection Check

### Architecture Understanding
1. Why use a buffer (Kafka) between collection and processing?

2. Logs arrive 10 minutes late. What detection capabilities are lost?

3. You're logging 100GB/day. How do you reduce costs without losing security visibility?

### Operational Thinking
1. A new application is deployed. What steps ensure its logs are properly collected and parsed?

2. Your SIEM shows gaps in log ingestion. How do you identify which systems aren't sending?

3. Analysts complain about too many alerts. How do you approach tuning?

---

## 5. Key Misconceptions

**MYTH: "More logs = better security"**
REALITY: More logs = more cost and noise. Targeted, high-value logging with good correlation beats volume.

**MYTH: "SIEM solves security"**
REALITY: SIEM is a tool. Detection rules, tuning, and analysts make it effective.

**MYTH: "Real-time means instant"**
REALITY: Collection, transport, and processing add latency. "Near real-time" is realistic.

**MYTH: "Once parsed, logs are ready"**
REALITY: Normalization, enrichment, and context are needed for effective analysis.

**MYTH: "Open source SIEM is free"**
REALITY: Infrastructure, expertise, and maintenance have real costs.

---

## 6. Completion Criteria

This module is complete when you can:

1. **Diagram** a complete logging pipeline
2. **Explain** normalization and why it matters
3. **Compare** major SIEM platforms
4. **Design** basic correlation rules
5. **Identify** common centralized logging challenges

---

## 7. Further Depth (Optional)

*   **Lab:** Set up ELK stack with sample logs
*   **Practice:** Write Splunk SPL or Elastic KQL queries
*   **Research:** Sigma rules for cross-platform detection
*   **Architecture:** Study high-volume logging at scale
