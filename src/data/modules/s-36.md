# AI-Assisted Threat Intelligence

## 1. Orientation

### What this module covers
Artificial Intelligence (AI) and Large Language Models (LLMs) are transforming how we process Threat Intelligence. This module separates the hype from the reality. We explore how to use AI for **data enrichment**, **massive report summarization**, and **pattern matching**, while critically examining the risks of **hallucinations** and **data leakage**.

### Fit in the Learning Path
As an Advanced Sentinel, the volume of data you face is unmanageable by humans alone. You previously learned to manually climb the Pyramid of Pain (S-14). Now, you will learn to use AI as a force multiplier to climb it faster, automating the "boring" parts of collection so you can focus on analysis.

### Learning Objectives
*   Leverage LLMs to **summarize** complex threat reports and extract TTPs.
*   Understand the **privacy risks** of using public AI models with TLP:RED data.
*   Explore **Machine Learning (ML)** use cases in malware clustering.
*   Write effective **prompts** for CTI tasks.

---

## 2. Core Content

### The AI Advantage in CTI
AI is not a replacement for analysts; it is an exoskeleton.

**1. Volume Processing (The "Too Much Reading" Problem)**
*   **Challenge:** 50 new threat reports are published daily. You cannot read them all.
*   **AI Solution:** "Summarize this PDF. List all Threat Actors, Target Sectors, and IOCs in a JSON format."
*   **Tech Stack:** OpenAI API, LangChain, local LLMs (Llama 3, Mistral).

**2. Pattern Recognition & Correlation**
*   **Challenge:** Connecting a dot in Log A with a dot in Log B when they look different.
*   **AI Solution:** Vector Embeddings. Converting text logs into mathematical vectors allows finding "semantically similar" events even if keywords don't match.
    *   *Example:* "User reset password" and "Credential update initiated" might not match a regex, but they are semantically close in vector space.

**3. Malware Classification (Clustering)**
*   **Traditional:** Hashing (Exact match).
*   **AI/ML:** Features extraction (File size, entropy, API calls).
*   **Result:** Grouping unknown files: "This looks 98% like Emotet, even though the hash is new."

### The Danger Zone: Risks & Ethics

#### 1. Hallucination (Fabricated Intel)
LLMs are probabilistic, not deterministic. They guess the next word.
*   **Risk:** You ask "What groups target the Energy sector?"
*   **Bad Output:** "APT-99 (The Electric Bears) targets energy." -> *APT-99 doesn't exist.*
*   **Mitigation:** **RAG (Retrieval-Augmented Generation)**. Force the AI to answer *only* using documents you provide, citing sources.

#### 2. Data Leakage (OPSEC Failure)
*   **Scenario:** You paste a proprietary incident response log into ChatGPT.
*   **Result:** You just trained the public model on your company's secrets.
*   **Rule:** **NEVER** paste PII, internal IP addresses, or TLP:RED data into a public model. Use local private instances or Enterprise-tier APIs with zero-retention policies.

---

## 3. Lab: Zero-Shot Extraction with AI

### Scenario
You have a raw unstructured text dump from a dark web forum. You need structured intelligence.

**Input Text:**
> "User xX_Hacker_Xx posted: Selling fresh RDP access to a US Hospital. 500 beds. Citrix Gateway. Price 2 BTC. Contact me on Jabber: [redacted]."

### Task 1: Prompt Engineering
Write a prompt to extract key fields.

**Bad Prompt:**
> "Read this."

**Good CTI Prompt:**
> "Act as a Threat Intelligence Analyst. Extract the following entities from the text below and output valid JSON:
> - Target_Sector
> - Access_Type
> - Price
> - Threat_Actor_Handle
>
> Text: [Insert Text]"

### Task 2: AI Output Analysis (Simulation)
**Output:**
```json
{
  "Target_Sector": "Healthcare (US Hospital)",
  "Access_Type": "RDP via Citrix Gateway",
  "Price": "2 BTC",
  "Threat_Actor_Handle": "xX_Hacker_Xx"
}
```
**Analyst Check:** Does the output match the input? Yes. The "500 beds" context helps estimate the victim size (Strategic Intel).

---

## 4. Advanced: Deception & Adversarial AI

### Adversarial Examples (Poisoning)
Attackers can "trick" ML models.
*   **Evasion:** Adding "goodware" strings to a malware file to shift its classification score just enough to bypass the AI AV.
*   **Implication:** We cannot blindly trust AI detection scores.

### AI for Attackers
*   **Polymorphic Code:** using AI to rewrite malware on the fly to change hashes/signatures.
*   **Phishing:** Perfect grammar, context-aware emails generated at scale.

---

## 5. Reflection & Tradecraft

1.  **Trust but Verify:** AI is a junior analyst. Review its work. Check the citations.
2.  **Privacy First:** If you can't run it locally (Ollama/LM Studio), sanitize the data before sending it to the cloud.
3.  **Prompt Libraries:** Build a repo of "Golden Prompts" for your team (e.g., "De-obfuscate this PowerShell script explain functionality").

---

## 6. Completion Criteria
1.  **Write** a prompt that extracts IOCs from unstructured text.
2.  **Explain** why "Hallucination" is dangerous in intelligence work.
3.  **Identify** the OPSEC failure of pasting internal logs into a public chatbot.
4.  **Differentiate** between standard Keyword Search and Vector/Semantic Search.

---

## 7. Further Resources
*   **Tools:** Ollama (Run local LLMs), PrivateGPT.
*   **Reading:** "Prompt Engineering for Cyber Security" (Online guides).
*   **Concept:** Retrieval Augmented Generation (RAG) for CTI.
