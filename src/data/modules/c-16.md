# What Makes a Good Detection

## 1. Orientation

### What this module covers
This module covers detection quality—the characteristics that separate a great detection rule from one that wastes analyst time or misses actual attacks. Understanding precision, recall, and resilience is essential for effective detection engineering.

### Fit in the Learning Path
Writing rules is easy. Writing good rules is hard. A bad rule creates noise, burns out analysts, and may still miss attacks. Understanding what makes a detection good is fundamental to detection engineering.

### Learning Objectives
By the end of this module, you will:
*   Evaluate detection quality using precision, recall, and resilience
*   Avoid common detection anti-patterns
*   Design rules that balance coverage and noise
*   Create actionable, documented, testable detections

---

## 2. Core Content

### Mental Model: The Detection Quality Triangle

**How professionals think about this:**
Every detection rule exists in tension between three properties: precision (low false positives), recall (low false negatives), and resilience (hard to evade). Optimizing for all three is the detection engineer's challenge.

```
THE DETECTION QUALITY TRIANGLE:

                    PRECISION
                   (Low False Positives)
                         ▲
                        /│\
                       / │ \
                      /  │  \
                     /   │   \
                    /    │    \
                   /  IDEAL │   \
                  /   ZONE  │    \
                 /          │     \
                ▼───────────┼──────►
            RECALL     RESILIENCE
         (Low False    (Hard to
          Negatives)    Evade)

TRADE-OFFS:
─────────────────────────────
High Precision + High Recall = Easy to evade
High Precision + High Resilience = May miss variants
High Recall + High Resilience = Noisy

GOAL: Maximize all three, accept trade-offs
```

### Detection Quality Metrics

**Measuring rule effectiveness:**

```
DETECTION METRICS:

TRUE POSITIVE (TP)
─────────────────────────────
Rule fires on actual attack
GOOD - This is what we want

FALSE POSITIVE (FP)
─────────────────────────────
Rule fires on legitimate activity
BAD - Wastes analyst time, causes alert fatigue

TRUE NEGATIVE (TN)
─────────────────────────────
Rule doesn't fire on legitimate activity
GOOD - Appropriate silence

FALSE NEGATIVE (FN)
─────────────────────────────
Rule doesn't fire on actual attack
BAD - Attack missed

CALCULATIONS:
─────────────────────────────
Precision = TP / (TP + FP)
"When it alerts, how often is it real?"

Recall = TP / (TP + FN)
"Of all actual attacks, how many do we catch?"

F1 Score = 2 × (Precision × Recall) / (Precision + Recall)
"Balanced measure of both"

EXAMPLE:
─────────────────────────────
100 alerts fired
80 true positives
20 false positives
10 attacks happened total (missed 2)

Precision = 80/(80+20) = 80%
Recall = 8/10 = 80%
```

### The Pyramid of Pain

**Detection resilience framework:**

```
THE PYRAMID OF PAIN (David Bianco):

                    ┌───────┐
                    │ TTPs  │  HARD to change
                    │       │  (How they attack)
                 ┌──┴───────┴──┐
                 │    Tools    │  Moderate
                 │             │  (What they use)
              ┌──┴─────────────┴──┐
              │ Network/Host      │
              │ Artifacts         │  Easier
           ┌──┴───────────────────┴──┐
           │    Domain Names         │
        ┌──┴─────────────────────────┴──┐
        │        IP Addresses           │  Easy
     ┌──┴───────────────────────────────┴──┐
     │           Hash Values               │  TRIVIAL to change
     └─────────────────────────────────────┘

DETECTION IMPLICATIONS:
─────────────────────────────
Hash-based detection: Least resilient
  - Attacker changes one byte, hash changes
  - Still catches commodity malware

Behavioral detection: Most resilient
  - Attacker must change entire technique
  - Harder to evade, may have more FPs

AIM HIGHER IN THE PYRAMID
Detect behaviors, not signatures
```

### Detection Anti-Patterns

**Common mistakes to avoid:**

```
DETECTION ANTI-PATTERNS:

1. FILENAME-BASED DETECTION
   ─────────────────────────────
   Bad: Alert if filename = "mimikatz.exe"
   Reality: Attacker renames to "update.exe"
   Better: Alert on LSASS access behavior
   
2. HASH-ONLY DETECTION
   ─────────────────────────────
   Bad: Alert if MD5 = [known-bad hash]
   Reality: Recompile changes hash
   Better: Hash for IOC enrichment, not primary

3. OVERLY SPECIFIC RULES
   ─────────────────────────────
   Bad: Alert if CommandLine = exact string
   Reality: Attackers add spaces, change case
   Better: Use contains, regex with tolerance

4. OVERLY BROAD RULES
   ─────────────────────────────
   Bad: Alert if PowerShell runs
   Reality: PowerShell runs constantly
   Better: Specific suspicious behaviors

5. NO TUNING STRATEGY
   ─────────────────────────────
   Bad: Rule deployed, never tuned
   Reality: FP rate grows, gets disabled
   Better: Build in exclusion mechanism

6. NO DOCUMENTATION
   ─────────────────────────────
   Bad: Rule exists, no one knows why
   Reality: Analyst doesn't know what to do
   Better: Embedded runbook link
```

### Characteristics of Great Detections

**What excellence looks like:**

```
GREAT DETECTION CHARACTERISTICS:

1. ACTIONABLE
   ─────────────────────────────
   Analyst knows exactly what to do
   Clear escalation path
   Defined response actions
   
   Bad: "Suspicious Activity Detected"
   Good: "Potential Credential Dumping - 
         Verify process legitimacy,
         check for lateral movement,
         escalate if confirmed"

2. DOCUMENTED
   ─────────────────────────────
   - What it detects (MITRE mapping)
   - Why it matters (impact)
   - Known false positives
   - Tuning guidance
   - Runbook link
   - Author and date

3. TESTED
   ─────────────────────────────
   Validated against:
   - Real attack techniques
   - Known benign activity
   - Edge cases
   - Evasion attempts

4. CONTEXT-RICH
   ─────────────────────────────
   Alert includes:
   - Relevant process tree
   - User context
   - Historical baseline deviation
   - Related events

5. TUNABLE
   ─────────────────────────────
   Built-in exclusion capability
   Easy to add false positive filters
   Documented tuning process

6. PERFORMANT
   ─────────────────────────────
   Doesn't kill SIEM performance
   Efficient query logic
   Appropriate time windows
```

### Detection Evaluation Framework

**Rating detection quality:**

| Criterion | Poor (1) | Fair (2) | Good (3) | Excellent (4) |
|-----------|----------|----------|----------|---------------|
| **Precision** | >50% FP | 25-50% FP | 10-25% FP | <10% FP |
| **Recall** | Catches <25% | Catches 25-50% | Catches 50-80% | Catches >80% |
| **Resilience** | Trivial bypass | Easy bypass | Moderate bypass | Hard bypass |
| **Actionability** | Unclear response | Partial guidance | Clear steps | Full runbook |
| **Documentation** | None | Minimal | Adequate | Comprehensive |
| **Testing** | Untested | Basic test | Attack simulation | Purple team validated |

### Detection Development Process

**Building quality detections:**

```
DETECTION DEVELOPMENT LIFECYCLE:

1. HYPOTHESIS
   ─────────────────────────────
   "Attackers doing X will generate Y telemetry"
   Base on: Threat intel, MITRE, incident experience
   
2. DATA ASSESSMENT
   ─────────────────────────────
   Do we have the data?
   Is it reliable?
   What fields are needed?
   
3. INITIAL RULE
   ─────────────────────────────
   Write first version
   Start broad, we'll tune down
   
4. BASELINE TESTING
   ─────────────────────────────
   Run against historical data
   What's the noise level?
   Sample false positives
   
5. ATTACK SIMULATION
   ─────────────────────────────
   Purple team validation
   Does it catch the attack?
   Try variations
   
6. TUNING
   ─────────────────────────────
   Add exclusions for FPs
   Adjust thresholds
   Optimize performance
   
7. DOCUMENTATION
   ─────────────────────────────
   Write runbook
   Document known FPs
   Add MITRE mapping
   
8. DEPLOYMENT
   ─────────────────────────────
   Enable in production
   Monitor initial performance
   
9. MAINTENANCE
   ─────────────────────────────
   Regular review (quarterly)
   Update for new evasion
   Retire if obsolete
```

### Example Detection Analysis

**Comparing detection quality:**

```
EXAMPLE: DETECTING MIMIKATZ

RULE 1 (BAD): Filename Match
─────────────────────────────
Condition: filename = "mimikatz.exe"

Precision: High (if fires, probably bad)
Recall: Low (trivial rename bypasses)
Resilience: None (one-byte change evades)

Verdict: ✗ Insufficient


RULE 2 (BETTER): Hash Match
─────────────────────────────
Condition: hash IN [known mimikatz hashes]

Precision: Very High
Recall: Medium (catches known versions)
Resilience: Low (recompile bypasses)

Verdict: ✓ Layered use only


RULE 3 (GOOD): Process Access to LSASS
─────────────────────────────
Condition: GrantedAccess contains 0x10
           AND TargetImage ends with lsass.exe
           AND SourceImage NOT IN [allowed list]

Precision: Medium (some legitimate tools)
Recall: High (must access LSASS for creds)
Resilience: High (can't avoid LSASS access)

Verdict: ✓✓ Strong detection


RULE 4 (BEST): Layered Approach
─────────────────────────────
Rule A: LSASS access (high confidence, any tool)
Rule B: Known hash (immediate block)
Rule C: Suspicious command line patterns
Rule D: Behavioral sequence (recon → dump → move)

Defense in depth for this technique
```

---

## 3. Guided Practice

### Exercise 1: Detection Critique

Rate these detection rules:

| Rule | Precision | Recall | Resilience | Overall |
|------|-----------|--------|------------|---------|
| Alert if filename = "evil.exe" | ___ | ___ | ___ | ___ |
| Alert if PowerShell runs | ___ | ___ | ___ | ___ |
| Alert if process accesses LSASS with 0x10 | ___ | ___ | ___ | ___ |

### Exercise 2: Improve a Rule

Original rule: "Alert if cmd.exe runs"

Problems:
1. ___
2. ___

Improved rule:
- Condition: ___
- Precision improvement: ___
- Still catches: ___

### Exercise 3: Detection Documentation

For a credential dumping detection, write:

- MITRE mapping: ___
- Alert title: ___
- Description: ___
- Analyst action: ___
- Known FPs: ___

---

## 4. Reflection Check

### Quality Understanding
1. Rule fires 100 times/day, 2 are real. Keep or disable?

2. Rule never fires. Good (no attacks) or bad (too specific)?

3. Rule catches 100% of attacks but 50% are FP. Acceptable?

### Strategic Thinking
1. Can only write 10 rules. Prioritization approach?

2. Perfect rule in lab fails in production. Root cause?

3. Analyst says "too noisy." Tune or educate?

---

## 5. Key Misconceptions

**MYTH: "More rules = better detection"**
REALITY: Quality over quantity. Ten great rules beat 100 noisy ones.

**MYTH: "Zero false positives is the goal"**
REALITY: Some FP is acceptable if recall is high. Balance matters.

**MYTH: "Once deployed, rules work forever"**
REALITY: Attackers adapt. Rules require continuous maintenance.

**MYTH: "Automated rules / vendor rules are sufficient"**
REALITY: Custom rules for your environment add significant value.

**MYTH: "High alert volume means good detection"**
REALITY: High volume often means noise. True positive rate matters.

---

## 6. Completion Criteria

This module is complete when you can:

1. **Evaluate** detections using precision, recall, resilience
2. **Identify** common detection anti-patterns
3. **Design** detections higher in the Pyramid of Pain
4. **Document** detections with actionable runbooks
5. **Maintain** detections through regular review

---

## 7. Further Depth (Optional)

*   **Framework:** Pyramid of Pain by David Bianco
*   **Resource:** Detection Engineering Weekly newsletter
*   **Practice:** Evaluate and improve 5 existing rules
*   **Reading:** MITRE ATT&CK data sources mapping
