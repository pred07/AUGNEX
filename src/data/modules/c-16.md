# What Makes a Good Detection

## 1. Orientation

### What this module covers
This module covers **Detection Quality**. Not all rules are created equal.

### Fit in the Learning Path
A bad rule wakes you up at 3am for nothing. A good rule saves the company.

### Learning Objectives
By the end of this module, you should understand:
*   **Precision** (Low False Positives).
*   **Recall** (Low False Negatives).
*   **Resilience** (Hard to evade).

---

## 2. Core Content

### The Triangle of Pain
1.  **Broad**: "Detect all defined badness". (High Noise).
2.  **Precise**: "Detect exact hash of Mimikatz". (Easy to evade).
3.  **Resilient**: "Detect LSASS memory access". (Hard to evade, but might catch legitimate AV tools).

### Characteristics of Great Rules
*   **Actionable**: The analyst knows exactly what to do.
*   **Documented**: Includes a Runbook link.
*   **Tested**: Validated against real attack data.

---

## 3. Guided Practice

### Rating a Rule
**Rule**: "Alert if filename is `mimikatz.exe`".
*   *Precision*: High (If it fires, it's bad).
*   *Resilience*: Zero (Attacker renames file to `mimi.exe`).
*   *Verdict*: **Bad Rule**.

**Rule**: "Alert if Process grants `SeDebugPrivilege` and opens handle to `lsass.exe`".
*   *Precision*: Medium (Some debuggers do this).
*   *Resilience*: High (Attacker *must* do this to dump creds).
*   *Verdict*: **Good Rule** (with tuning).

---

## 4. Reflection Check

1.  **Maintenance**: Rules rot. (Review them every 6 months).
2.  **Performance**: Does the rule kill the SIEM CPU? (Avoid regex on raw text).
3.  **Context**: Alert title should be "Credential Dumping Suspected", not "Rule ID 505 fired".

---

## 5. Completion Criteria

This module is complete when:
1.  You can critique a rule for **Resilience**.
2.  You stop writing file-name based detections.
3.  You demand a **Runbook** for every Alert.
