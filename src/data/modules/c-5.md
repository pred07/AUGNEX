# Detection as an Engineering Problem

## 1. Orientation

### What this module covers
This module treats detection as software engineering—applying version control, CI/CD pipelines, testing frameworks, and deployment automation to security detection rules. Detection engineering brings software development rigor to SOC operations.

### Fit in the Learning Path
Ad-hoc rule creation leads to configuration drift, untested detections, and knowledge silos. Treating detection as code brings reproducibility, testability, and collaboration to security operations. This is the modern standard for mature security teams.

### Learning Objectives
By the end of this module, you will:
*   Apply version control to detection rules
*   Implement CI/CD pipelines for rule deployment
*   Test detection rules before production
*   Manage detection rules as software assets

---

## 2. Core Content

### Mental Model: Detection is Code

**How professionals think about this:**
If you don't edit production servers by hand, why edit production SIEM rules by hand? Detection rules are code that protects the organization. They deserve the same rigor as application code.

```
DETECTION AS CODE CONCEPT:

TRADITIONAL APPROACH (fragile):
┌─────────────────────────────────────────────────────────────────────┐
│                                                                     │
│  Analyst thinks of rule → Opens SIEM console → Types rule          │
│       → Saves and hopes it works → No documentation                │
│       → Analyst leaves company → Rule breaks → No one knows why    │
│                                                                     │
│  Problems:                                                         │
│  - No version history                                              │
│  - No testing                                                      │
│  - No review process                                               │
│  - Configuration drift                                             │
│  - Knowledge loss                                                  │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘

ENGINEERING APPROACH (reliable):
┌─────────────────────────────────────────────────────────────────────┐
│                                                                     │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐     │
│  │  Write   │──► │  Commit  │──► │   Test   │──► │  Deploy  │     │
│  │  (IDE)   │    │  (Git)   │    │  (CI/CD) │    │  (Auto)  │     │
│  └──────────┘    └──────────┘    └──────────┘    └──────────┘     │
│       │               │               │               │            │
│       ▼               ▼               ▼               ▼            │
│  Rule in YAML    History +       True Positive    Production       │
│  Sigma format    Review           Validated        SIEM            │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### The Detection Engineering Pipeline

**End-to-end workflow:**

```
DETECTION CI/CD PIPELINE:

1. DEVELOPMENT
   ─────────────────────────────
   Analyst writes rule locally
   Format: Sigma YAML
   IDE: VS Code with Sigma extension
   
   rules/windows/process_creation/
     └── mimikatz_execution.yml

2. VERSION CONTROL
   ─────────────────────────────
   git add rules/windows/...
   git commit -m "Add Mimikatz detection rule"
   git push origin feature/mimikatz-rule
   
   Creates Pull Request for review

3. PEER REVIEW
   ─────────────────────────────
   Senior analyst reviews:
   - Logic correct?
   - False positive rate?
   - MITRE mapping accurate?
   - Documentation complete?

4. AUTOMATED TESTING
   ─────────────────────────────
   CI pipeline triggers:
   a) Syntax validation
   b) Run against attack dataset
   c) Check for detections
   d) Check false positive rate
   
   Pass → Merge allowed
   Fail → Reject, fix required

5. DEPLOYMENT
   ─────────────────────────────
   Merged to main → Automatically:
   - Convert Sigma to SIEM-native format
   - Deploy to Splunk/Sentinel/Elastic
   - Enable in production
   
6. MONITORING
   ─────────────────────────────
   Track in production:
   - Alert volume
   - True/false positive ratio
   - Analyst feedback
```

### Sigma Rule Format

**The universal detection language:**

```yaml
# Example Sigma Rule

title: Mimikatz Command Line Execution
id: a5b9ce32-1234-5678-abcd-ef0123456789
status: stable
description: Detects Mimikatz execution via command line arguments
author: Detection Engineering Team
date: 2024/01/15
modified: 2024/01/20
references:
    - https://attack.mitre.org/techniques/T1003/001/
tags:
    - attack.credential_access
    - attack.t1003.001
logsource:
    category: process_creation
    product: windows
detection:
    selection_cmd:
        CommandLine|contains:
            - 'sekurlsa'
            - 'kerberos::list'
            - 'crypto::certificates'
            - 'lsadump'
    condition: selection_cmd
falsepositives:
    - Legitimate security testing
    - Incident response tools
level: high
```

**Sigma advantages:**

| Feature | Benefit |
|---------|---------|
| **SIEM-agnostic** | Write once, deploy to any platform |
| **Version controlled** | Track changes over time |
| **Human readable** | Easy to review and understand |
| **Community shared** | Thousands of rules available |
| **Testable** | Run against datasets |

### Testing Detection Rules

**Automated testing approaches:**

```
DETECTION TESTING FRAMEWORK:

1. SYNTAX TESTING
   ─────────────────────────────
   sigmac --lint rule.yml
   
   Checks:
   - Valid YAML syntax
   - Required fields present
   - Correct log source

2. UNIT TESTING
   ─────────────────────────────
   Test against known-good samples
   
   Test case (JSON/EVTX):
   {
     "process": "mimikatz.exe",
     "cmdline": "sekurlsa::logonpasswords"
   }
   
   Expected: Detection triggers
   
3. INTEGRATION TESTING
   ─────────────────────────────
   Run against real attack dataset
   
   pySigma test:
   - Atomic Red Team logs
   - EVTX samples
   - Custom attack logs

4. FALSE POSITIVE TESTING
   ─────────────────────────────
   Run against production-like dataset
   
   FP dataset:
   - Normal Windows logs
   - Legitimate admin activity
   - Known benign patterns
   
   Expected: No detections

5. REGRESSION TESTING
   ─────────────────────────────
   After changes, verify:
   - Existing detections still work
   - No new false positives
```

### Git Workflow for Detections

**Practical workflow:**

```
DETECTION GIT WORKFLOW:

Repository Structure:
─────────────────────────────
detection-rules/
├── rules/
│   ├── windows/
│   │   ├── process_creation/
│   │   ├── registry/
│   │   └── network_connection/
│   ├── linux/
│   └── cloud/
├── tests/
│   ├── attack_samples/
│   └── benign_samples/
├── pipelines/
│   └── deploy.yml
└── README.md

Branching Strategy:
─────────────────────────────
main          Production rules (protected)
├── develop   Integration branch
│   ├── feature/phishing-detection
│   ├── feature/lateral-movement
│   └── hotfix/fix-false-positive

Workflow:
─────────────────────────────
1. git checkout -b feature/new-rule
2. Write rule, write test
3. git add . && git commit
4. git push origin feature/new-rule
5. Create Pull Request
6. Pass CI + Review → Merge
7. Auto-deploy to SIEM
```

### Benefits of Detection as Code

**Operational improvements:**

```
DETECTION AS CODE BENEFITS:

1. HISTORY & ACCOUNTABILITY
   ─────────────────────────────
   "Who changed this rule?"
   git blame rules/windows/mimikatz.yml
   
   "When was it changed?"
   git log --oneline rules/windows/mimikatz.yml
   
   "Why was it changed?"
   Commit message + PR description

2. ROLLBACK CAPABILITY
   ─────────────────────────────
   "New rule broke SIEM!"
   git revert abc123
   
   Instant recovery to known-good state

3. COLLABORATION
   ─────────────────────────────
   - Multiple analysts work in parallel
   - Pull requests enable review
   - Comments document decisions
   - Merge conflicts prevent overwrites

4. CONSISTENCY
   ─────────────────────────────
   - Same rules across all SIEMs
   - No "this one was edited manually"
   - Single source of truth

5. TESTING
   ─────────────────────────────
   - Rules validated before production
   - Known-bad samples confirm detection
   - Known-good samples confirm no FP
   
6. DOCUMENTATION
   ─────────────────────────────
   - Rule files ARE documentation
   - MITRE mapping included
   - References included
   - Change history preserved
```

### Multi-SIEM Deployment

**Supporting heterogeneous environments:**

```
SIGMA CONVERSION PIPELINE:

          ┌─────────────────┐
          │  Sigma Rule     │
          │  (YAML)         │
          └────────┬────────┘
                   │
          ┌────────▼────────┐
          │    sigmac       │
          │  (converter)    │
          └────────┬────────┘
                   │
     ┌─────────────┼─────────────┐
     │             │             │
     ▼             ▼             ▼
┌────────┐   ┌────────┐   ┌────────┐
│ Splunk │   │ Azure  │   │ Elastic│
│  SPL   │   │Sentinel│   │  KQL   │
└────────┘   └────────┘   └────────┘

One rule → All platforms

Commands:
sigmac -t splunk rule.yml > rule.spl
sigmac -t elasticsearch rule.yml > rule.json
sigmac -t microsoft365defender rule.yml > rule.kql
```

### Metrics and Feedback

**Continuous improvement:**

| Metric | Indicates | Action |
|--------|-----------|--------|
| **Alert volume** | Rule activity | Tune if too noisy |
| **TP rate** | Rule accuracy | Improve if low |
| **Mean time to deploy** | Pipeline efficiency | Optimize if slow |
| **Rules deployed** | Coverage | Increase over time |
| **Rules reverted** | Quality issues | Improve testing |

---

## 3. Guided Practice

### Exercise 1: Write a Sigma Rule

Write a Sigma rule for detecting suspicious PowerShell download cradle:

```yaml
title: ___
logsource:
    category: ___
    product: ___
detection:
    selection:
        CommandLine|contains:
            - ___
            - ___
    condition: selection
level: ___
```

### Exercise 2: Git Workflow

A false positive is reported. Document your workflow:

1. Create branch: `git checkout -b ___`
2. Edit rule: ___
3. Test change: ___
4. Commit: `git commit -m "___"`
5. Deploy: ___

### Exercise 3: CI/CD Pipeline

Design a pipeline for detection rules:

| Stage | Action | Pass Criteria |
|-------|--------|---------------|
| Lint | ___ | ___ |
| Test | ___ | ___ |
| Deploy | ___ | ___ |

---

## 4. Reflection Check

### Engineering Understanding
1. Why is clicking "Save" in the SIEM console a problem?

2. Rule was edited in console AND in Git. What happens?

3. CI test passes but rule fails in production. Why?

### Practical Thinking
1. Analyst doesn't know Git. Is Detection as Code worth it?

2. SIEM doesn't support API deployment. Workaround?

3. 1000 existing rules in SIEM, none in Git. Migration plan?

---

## 5. Key Misconceptions

**MYTH: "Detection as Code is overkill for small teams"**
REALITY: Small teams benefit most from automation and documentation.

**MYTH: "It slows down detection deployment"**
REALITY: Initial setup takes time; long-term it's much faster and safer.

**MYTH: "Analysts shouldn't need to learn Git"**
REALITY: Modern security requires engineering skills. Git is fundamental.

**MYTH: "Our SIEM doesn't support this"**
REALITY: All major SIEMs have APIs. Sigma supports most platforms.

**MYTH: "Testing detections is too hard"**
REALITY: Atomic Red Team and log samples make testing practical.

---

## 6. Completion Criteria

This module is complete when you can:

1. **Write** detection rules in Sigma format
2. **Version control** rules using Git
3. **Test** rules against attack samples
4. **Deploy** rules via CI/CD pipeline
5. **Explain** benefits of Detection as Code

---

## 7. Further Depth (Optional)

*   **Tool:** Sigma (https://github.com/SigmaHQ/sigma)
*   **Practice:** Set up detection pipeline with GitHub Actions
*   **Resource:** Atomic Red Team for test samples
*   **Reading:** Detection Engineering blog posts
