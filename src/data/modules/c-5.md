# Detection as an Engineering Problem

## 1. What this module covers
Moving away from "writing rules" to "Detection Engineering". Applying software engineering principles (Version Control, Testing, CI/CD) to detection logic.

## 2. Why this matters for purple teams
If your detections are brittle (break easily) or noisy (too many alerts), the SOC ignores them. Engineering creates robust detections.

## 3. Core concepts explained clearly

### The Detection Lifecycle
1.  **Requirement**: "Detect Mimikatz".
2.  **Research**: How does Mimikatz work? (Memory injection, specific command lines).
3.  **Development**: Write the query (SPL/YARA).
4.  **Testing**: Run the attack. Does it fire?
5.  **Deployment**: Push to SIEM.
6.  **Tuning**: Too false positive? Refine.

### Detection as Code (DaC)
Storing your SIEM rules in Git.
*   `rules/mimikatz_detection.yml`
*   Commits, Pull Requests, and Peer Reviews before pushing to the SIEM.

## 4. Validation examples

### Testing a Rule Logic
*   **Objective**: Detect specific PowerShell download cradle.
*   **Draft Rule**: `CommandLine="*Net.WebClient*"`
*   **Test**: Run a legit admin script that uses WebClient.
*   **Result**: False Positive!
*   **Refinement**: `CommandLine="*Net.WebClient*" AND CommandLine="*Hidden*"`
*   **Test**: Run legit script (Pass). Run Attack (Alert).

## 5. Common gaps and failures
*   **Hardcoding**: Writing rules for "IP 1.2.3.4". The IP changes tomorrow. Rule fails. Write rules for *behaviors* (TTPs), not *indicators* (IOCs).
*   **No Versioning**: Someone changes a rule and breaks it. No history to rollback.

## 6. Key takeaways
*   **lifecycle**: Detections are software. Treat them that way.
*   **Maintenance**: A rule that hasn't fired in 2 years is dead code. Review it.
